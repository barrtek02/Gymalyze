{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738c41f6efd9b259",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:50:24.889712Z",
     "start_time": "2024-10-04T10:50:20.811976Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.utils.pose_estimator import PoseEstimator\n",
    "\n",
    "pose_estimation = PoseEstimator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b6001b6695616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "659bdb8b1dee6795",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:51:25.814261Z",
     "start_time": "2024-10-04T10:50:24.945252Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "from src.utils.video_processor import VideoProcessor\n",
    "\n",
    "landmarks = VideoProcessor.process_video(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\videos\\test\\test\\squat\\squat_1.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d2608a5d154b251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T18:44:54.656526Z",
     "start_time": "2024-10-02T18:44:37.320972Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from src.utils.pose_estimator import PoseEstimator\n",
    "\n",
    "class VideoProcessor2:\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_frame(frame, pose_estimator):\n",
    "        \"\"\"\n",
    "        Process a single frame to extract pose landmarks.\n",
    "\n",
    "        :param frame: A single video frame.\n",
    "        :param pose_estimator: PoseEstimator instance for detecting landmarks.\n",
    "        :return: List of pose landmarks for the frame.\n",
    "        \"\"\"\n",
    "        pose_landmarks = pose_estimator.estimate_pose(frame)\n",
    "        if pose_landmarks:\n",
    "            return [[lm.x, lm.y, lm.z, lm.visibility] for lm in pose_landmarks.landmark]\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def process_video(video_path: Path, num_threads=4) -> list[list[list[float]]]:\n",
    "        \"\"\"\n",
    "        Process a single video file using multiple threads to extract pose landmarks.\n",
    "\n",
    "        :param video_path: Path to the video file (as a Path object).\n",
    "        :param num_threads: Number of threads to use for parallel processing.\n",
    "        :return: A list of pose landmarks for each frame in the video.\n",
    "        \"\"\"\n",
    "        pose_estimator = PoseEstimator()  # Create a new PoseEstimator instance\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        all_landmarks = []\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Unable to open video file: {video_path}\")\n",
    "\n",
    "        # Store frames for processing\n",
    "        frames = []\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "\n",
    "        # Use threading for parallel frame processing\n",
    "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            futures = {executor.submit(VideoProcessor2.process_frame, frame, pose_estimator): frame for frame in frames}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    all_landmarks.append(result)\n",
    "\n",
    "        return all_landmarks\n",
    "\n",
    "\n",
    "\n",
    "landmarks2 = VideoProcessor2.process_video(\n",
    "    r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\videos\\test\\test\\squat\\squat_1.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81055a053668cfff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T08:11:43.071400Z",
     "start_time": "2024-10-03T08:11:20.983348Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'VideoProcessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 52\u001B[0m\n\u001B[0;32m     48\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m all_landmarks\n\u001B[0;32m     50\u001B[0m landmarks3 \u001B[38;5;241m=\u001B[39m VideoProcessor3\u001B[38;5;241m.\u001B[39mprocess_video(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mbarrt\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mPycharmProjects\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mGymalyze\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124msrc\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mvideos\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124msquat\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124msquat_1.mp4\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 52\u001B[0m \u001B[43mVideoProcessor\u001B[49m\u001B[38;5;241m.\u001B[39mclassify_video(landmarks3)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'VideoProcessor' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from src.utils.pose_estimator import PoseEstimator\n",
    "\n",
    "class VideoProcessor3:\n",
    "    @staticmethod\n",
    "    def process_video(video_path: Path) -> list[list[list[float]]]:\n",
    "        \"\"\"\n",
    "        Process a single video file, extract pose landmarks, and return them.\n",
    "\n",
    "        :param video_path: Path to the video file (as a Path object).\n",
    "        :return: A list of pose landmarks for each frame in the video.\n",
    "        \"\"\"\n",
    "        pose_estimator = PoseEstimator()  # Create a new PoseEstimator instance for each thread\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        all_landmarks = []\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Unable to open video file: {video_path}\")\n",
    "\n",
    "        def process_frame(frame):\n",
    "            try:\n",
    "                pose_landmarks = pose_estimator.estimate_pose(frame)\n",
    "                if pose_landmarks:\n",
    "                    return [\n",
    "                        [lm.x, lm.y, lm.z, lm.visibility]\n",
    "                        for lm in pose_landmarks.landmark\n",
    "                    ]\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping frame due to error: {e}\")\n",
    "            return None\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                futures.append(executor.submit(process_frame, frame))\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    all_landmarks.append(result)\n",
    "\n",
    "        cap.release()\n",
    "        return all_landmarks\n",
    "    \n",
    "landmarks3 = VideoProcessor3.process_video(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\videos\\test\\test\\squat\\squat_1.mp4')\n",
    "\n",
    "VideoProcessor.classify_video(landmarks3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d6604cd72aba63d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T19:00:41.148034Z",
     "start_time": "2024-10-02T19:00:40.795176Z"
    }
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "class FileVideoStream:\n",
    "    def __init__(self, path, queue_size=128):\n",
    "        # Initialize the video file stream and queue to store frames\n",
    "        self.stream = cv2.VideoCapture(path)\n",
    "        self.stopped = False\n",
    "        self.queue = Queue(maxsize=queue_size)\n",
    "    \n",
    "    def start(self):\n",
    "        # Start the thread to read frames from the video stream\n",
    "        t = Thread(target=self.update, args=())\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "        return self\n",
    "    \n",
    "    def update(self):\n",
    "        # Loop until the stream is stopped\n",
    "        while True:\n",
    "            if self.stopped:\n",
    "                return\n",
    "            \n",
    "            if not self.queue.full():\n",
    "                # Read the next frame from the file\n",
    "                (grabbed, frame) = self.stream.read()\n",
    "                \n",
    "                # If the frame was not grabbed, stop the thread\n",
    "                if not grabbed:\n",
    "                    self.stop()\n",
    "                    return\n",
    "                \n",
    "                self.queue.put(frame)\n",
    "    \n",
    "    def read(self):\n",
    "        # Return the next frame from the queue\n",
    "        return self.queue.get()\n",
    "    \n",
    "    def more(self):\n",
    "        # Return True if there are still frames in the queue\n",
    "        return self.queue.qsize() > 0\n",
    "    \n",
    "    def stop(self):\n",
    "        # Indicate that the thread should be stopped\n",
    "        self.stopped = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da51a35affbb488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T19:01:11.513412Z",
     "start_time": "2024-10-02T19:01:11.496257Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barrt\\AppData\\Local\\Temp\\ipykernel_25032\\2019216617.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('exercise_lstm_model_filtered.pth', map_location='cpu')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Load your trained model\u001B[39;00m\n\u001B[0;32m      9\u001B[0m model \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mexercise_lstm_model_filtered.pth\u001B[39m\u001B[38;5;124m'\u001B[39m, map_location\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 10\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval\u001B[49m()\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Define your label mapping\u001B[39;00m\n\u001B[0;32m     13\u001B[0m label_mapping \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m0\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSquat\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m1\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDeadlift\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m2\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBench Press\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m3\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPush-Up\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m4\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLat Pulldown\u001B[39m\u001B[38;5;124m'\u001B[39m}\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from imutils.video import FPS\n",
    "import time\n",
    "\n",
    "# Load your trained model\n",
    "model = torch.load('lstm_v1.pth', map_location='cpu')\n",
    "model.eval()\n",
    "\n",
    "# Define your label mapping\n",
    "label_mapping = {0: 'Squat', 1: 'Deadlift', 2: 'Bench Press', 3: 'Push-Up', 4: 'Lat Pulldown'}\n",
    "\n",
    "def preprocess_frame_for_model(frame):\n",
    "    # Your preprocessing code here (e.g., resizing, normalizing landmarks)\n",
    "    # Ensure the input is compatible with your model\n",
    "    return processed_frame\n",
    "\n",
    "# Video path\n",
    "video_path = \"path_to_your_video.mp4\"\n",
    "\n",
    "# Initialize video stream\n",
    "fvs = FileVideoStream(video_path).start()\n",
    "time.sleep(1.0)  # Allow buffer to fill\n",
    "\n",
    "fps = FPS().start()\n",
    "\n",
    "while fvs.more():\n",
    "    # Read the next frame from the video stream\n",
    "    frame = fvs.read()\n",
    "\n",
    "    # Preprocess the frame (extract landmarks or features compatible with your model)\n",
    "    processed_frame = preprocess_frame_for_model(frame)\n",
    "\n",
    "    # Convert to torch tensor\n",
    "    input_tensor = torch.tensor(processed_frame).float().unsqueeze(0)\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        _, predicted_label = torch.max(output, 1)\n",
    "        predicted_class = label_mapping[predicted_label.item()]\n",
    "\n",
    "    # Display the frame with the prediction\n",
    "    cv2.putText(frame, f\"Predicted: {predicted_class}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Break on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    fps.update()\n",
    "\n",
    "# Stop the FPS counter and clean up\n",
    "fps.stop()\n",
    "print(f\"[INFO] Elapsed time: {fps.elapsed():.2f}\")\n",
    "print(f\"[INFO] Approx. FPS: {fps.fps():.2f}\")\n",
    "\n",
    "fvs.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8cddd47f931ed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T19:04:10.572781Z",
     "start_time": "2024-10-02T19:04:10.567332Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class ThreadedVideoStream:\n",
    "    def __init__(self, path, queue_size=128):\n",
    "        # Initialize the video file stream and queue to store frames\n",
    "        self.stream = cv2.VideoCapture(str(path))\n",
    "        self.stopped = False\n",
    "        self.queue = Queue(maxsize=queue_size)\n",
    "\n",
    "    def start(self):\n",
    "        # Start the thread to read frames from the video stream\n",
    "        t = Thread(target=self.update, args=())\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "        return self\n",
    "\n",
    "    def update(self):\n",
    "        # Loop until the stream is stopped\n",
    "        while True:\n",
    "            if self.stopped:\n",
    "                return\n",
    "\n",
    "            if not self.queue.full():\n",
    "                # Read the next frame from the file\n",
    "                grabbed, frame = self.stream.read()\n",
    "\n",
    "                # If the frame was not grabbed, stop the thread\n",
    "                if not grabbed:\n",
    "                    self.stop()\n",
    "                    return\n",
    "\n",
    "                self.queue.put(frame)\n",
    "\n",
    "    def read(self):\n",
    "        # Return the next frame from the queue\n",
    "        return self.queue.get()\n",
    "\n",
    "    def more(self):\n",
    "        # Return True if there are still frames in the queue\n",
    "        return self.queue.qsize() > 0\n",
    "\n",
    "    def stop(self):\n",
    "        # Indicate that the thread should be stopped\n",
    "        self.stopped = True\n",
    "        self.stream.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329904f0a97c78c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf67a234c2102a43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T19:04:14.151892Z",
     "start_time": "2024-10-02T19:04:13.501136Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from src.utils.pose_estimator import PoseEstimator\n",
    "\n",
    "class VideoProcessor:\n",
    "\n",
    "    @staticmethod\n",
    "    def process_video(video_path: Path) -> list[list[list[float]]]:\n",
    "        \"\"\"\n",
    "        Process a single video file, extract pose landmarks, and return them.\n",
    "        :param video_path: Path to the video file (as a Path object).\n",
    "        :return: A list of pose landmarks for each frame in the video.\n",
    "        \"\"\"\n",
    "        pose_estimator = PoseEstimator()  # Create a PoseEstimator instance\n",
    "        all_landmarks = []\n",
    "        \n",
    "        # Initialize threaded video stream\n",
    "        video_stream = ThreadedVideoStream(video_path).start()\n",
    "\n",
    "        while video_stream.more():\n",
    "            frame = video_stream.read()\n",
    "\n",
    "            try:\n",
    "                pose_landmarks = pose_estimator.estimate_pose(frame)\n",
    "\n",
    "                if pose_landmarks:\n",
    "                    # Extract landmarks for the current frame\n",
    "                    all_landmarks.append(\n",
    "                        [\n",
    "                            [lm.x, lm.y, lm.z, lm.visibility]\n",
    "                            for lm in pose_landmarks.landmark\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping frame due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Stop the video stream\n",
    "        video_stream.stop()\n",
    "        return all_landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b9bf8e18c5eeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T19:04:57.819411200Z",
     "start_time": "2024-10-02T19:04:19.762103Z"
    }
   },
   "outputs": [],
   "source": [
    "landmarks = VideoProcessor.process_video(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\videos\\test\\test\\squat\\squat_1.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c55456693c68f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T19:05:49.213975Z",
     "start_time": "2024-10-02T19:05:49.104573Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -v VIDEO\n",
      "ipykernel_launcher.py: error: the following arguments are required: -v/--video\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\u001B[1;31m:\u001B[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import FPS\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-v\", \"--video\", required=True,\n",
    "                help=\"path to input video file\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# open a pointer to the video stream and start the FPS timer\n",
    "stream = cv2.VideoCapture(args[\"video\"])\n",
    "fps = FPS().start()\n",
    "\n",
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "    # grab the frame from the threaded video file stream\n",
    "    (grabbed, frame) = stream.read()\n",
    "\n",
    "    # if the frame was not grabbed, then we have reached the end of the stream\n",
    "    if not grabbed:\n",
    "        break\n",
    "\n",
    "    # resize the frame and convert it to grayscale (while still retaining 3 channels)\n",
    "    frame = imutils.resize(frame, width=450)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = np.dstack([frame, frame, frame])\n",
    "\n",
    "    # display a piece of text to the frame\n",
    "    cv2.putText(frame, \"Slow Method\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    # show the frame and update the FPS counter\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.waitKey(1)\n",
    "    fps.update()\n",
    "\n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "# do a bit of cleanup\n",
    "stream.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e57cd620a90151d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T08:12:55.519334Z",
     "start_time": "2024-10-03T08:12:28.083500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd4c3e4c9150cd2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T08:21:39.335131Z",
     "start_time": "2024-10-03T08:21:35.895353Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barrt\\AppData\\Local\\Temp\\ipykernel_16452\\3244513432.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('exercise_lstm_model_filtered.pth', map_location='cpu'))\n"
     ]
    }
   ],
   "source": [
    "from src.models.lstm import ExerciseLSTM\n",
    "import torch\n",
    "from src.utils.video_processor import VideoProcessor\n",
    "from src.utils.pose_estimator import PoseEstimator\n",
    "\n",
    "input_size = 132   # Should be 132\n",
    "hidden_size = 128                       # As used during training\n",
    "num_layers = 2                          # As used during training\n",
    "num_classes = 5                         # Set to 5 to match the trained model\n",
    "\n",
    "# Initialize the model\n",
    "model = ExerciseLSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Load the trained model weights\n",
    "model.load_state_dict(torch.load('lstm_v1.pth', map_location='cpu'))\n",
    "model.eval()\n",
    "video_proccesor = VideoProcessor(PoseEstimator(), model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2abadc85631b7ad0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:55:28.299492Z",
     "start_time": "2024-10-04T10:54:45.845009Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "class VideoProcessor3:\n",
    "    @staticmethod\n",
    "    def process_video(video_path: Path) -> list[list[list[float]]]:\n",
    "        \"\"\"\n",
    "        Process a single video file, extract pose landmarks, and return them.\n",
    "\n",
    "        :param video_path: Path to the video file (as a Path object).\n",
    "        :return: A list of pose landmarks for each frame in the video.\n",
    "        \"\"\"\n",
    "        pose_estimator = PoseEstimator()  # Create a new PoseEstimator instance for each thread\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        all_landmarks = []\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Unable to open video file: {video_path}\")\n",
    "\n",
    "        def process_frame(frame):\n",
    "            try:\n",
    "                pose_landmarks = pose_estimator.estimate_pose(frame)\n",
    "                if pose_landmarks:\n",
    "                    return [\n",
    "                        [lm.x, lm.y, lm.z, lm.visibility]\n",
    "                        for lm in pose_landmarks.landmark\n",
    "                    ]\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping frame due to error: {e}\")\n",
    "            return None\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                futures.append(executor.submit(process_frame, frame))\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    all_landmarks.append(result)\n",
    "\n",
    "        cap.release()\n",
    "        return all_landmarks\n",
    "    \n",
    "landmarks3 = VideoProcessor3.process_video(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\videos\\test\\test\\squat\\squat_1.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aad9f1d942f209e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T08:22:03.661075Z",
     "start_time": "2024-10-03T08:22:02.334872Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "LSTM: Expected input to be 2D or 3D, got 4D instead",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mvideo_proccesor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclassify_video\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlandmarks3\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Gymalyze\\src\\utils\\video_handling.py:166\u001B[0m, in \u001B[0;36mVideoProcessor.classify_video\u001B[1;34m(self, sequence)\u001B[0m\n\u001B[0;32m    163\u001B[0m sequence \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(sequence, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)  \u001B[38;5;66;03m# Add batch dimension\u001B[39;00m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 166\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequence\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    167\u001B[0m     _, predicted_class \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m predicted_class\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Gymalyze\\src\\models\\lstm.py:18\u001B[0m, in \u001B[0;36mExerciseLSTM.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     15\u001B[0m h0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers, x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_size)\u001B[38;5;241m.\u001B[39mto(x\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m     16\u001B[0m c0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers, x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_size)\u001B[38;5;241m.\u001B[39mto(x\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m---> 18\u001B[0m out, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mh0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc0\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m out \u001B[38;5;241m=\u001B[39m out[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]\n\u001B[0;32m     21\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(out)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:882\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m    880\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m):\n\u001B[1;32m--> 882\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLSTM: Expected input to be 2D or 3D, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mD instead\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    883\u001B[0m     is_batched \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m\n\u001B[0;32m    884\u001B[0m     batch_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mValueError\u001B[0m: LSTM: Expected input to be 2D or 3D, got 4D instead"
     ]
    }
   ],
   "source": [
    "video_proccesor.classify_video(landmarks3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b921f355bd5c9490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:53:09.946887Z",
     "start_time": "2024-10-04T10:53:03.663973Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VideoProcessor() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 34\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# ---------------------- Select a Random Sample ----------------------\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# Select a random index\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     31\u001B[0m \n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# ---------------------- Preprocess the Sample ----------------------p\u001B[39;00m\n\u001B[0;32m     33\u001B[0m poseestimator \u001B[38;5;241m=\u001B[39m PoseEstimator()\n\u001B[1;32m---> 34\u001B[0m videoprocessor \u001B[38;5;241m=\u001B[39m \u001B[43mVideoProcessor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mposeestimator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m sample_sequence \u001B[38;5;241m=\u001B[39m videoprocessor\u001B[38;5;241m.\u001B[39mprocess_video(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mbarrt\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mPycharmProjects\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mGymalyze\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124msrc\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mvideos\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124msquat\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124msquat_1.mp4\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# Ensure the sample is a NumPy array of type float32\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: VideoProcessor() takes no arguments"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from src.models.lstm import ExerciseLSTM\n",
    "from src.utils.pose_estimator import PoseEstimator\n",
    "from src.utils.video_processor import VideoProcessor\n",
    "\n",
    "# ---------------------- Load Data ----------------------\n",
    "\n",
    "# Load the flattened sequences and labels\n",
    "X = np.load(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\landmarks_data.npy', allow_pickle=True)  # Shape: (num_samples, seq_length, input_size)\n",
    "# y = np.load('labels_data.npy')  # Shape: (num_samples,)\n",
    "\n",
    "# Load the label mapping (ensure it's the one used during training)\n",
    "with open(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\label_mapping.json', 'r') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "# Create an inverse label mapping (from indices to class names)\n",
    "inverse_label_mapping = {int(v): k for k, v in label_mapping.items()}\n",
    "\n",
    "# ---------------------- Select a Random Sample ----------------------\n",
    "\n",
    "# Select a random index\n",
    "# idx = random.randint(0, len(X) - 1)\n",
    "# sample_sequence = X[idx]  # Shape: (sequence_length, input_size)\n",
    "# true_label = y[idx]  # This is the numerical label (should be between 0 and 4)\n",
    "\n",
    "# ---------------------- Preprocess the Sample ----------------------p\n",
    "poseestimator = PoseEstimator()\n",
    "videoprocessor = VideoProcessor(poseestimator)\n",
    "sample_sequence = videoprocessor.process_video(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\videos\\test\\test\\squat\\squat_1.mp4')\n",
    "# Ensure the sample is a NumPy array of type float32\n",
    "sample_sequence_for_testing = deepcopy(sample_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ba82c9c46d9ff31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T16:13:22.808465Z",
     "start_time": "2024-10-03T16:13:22.801221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 132)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list = np.array([np.array(sample).flatten() for sample in sample_sequence], dtype=np.float32)\n",
    "new_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae24fabf507b1b1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T16:13:22.841572Z",
     "start_time": "2024-10-03T16:13:22.835103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 132)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65fe1c72138a49e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T16:13:22.882269Z",
     "start_time": "2024-10-03T16:13:22.877065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 132)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d3156b4c1caf8f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:53:11.763880Z",
     "start_time": "2024-10-04T10:53:11.747073Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m sample_sequence \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\u001B[43msample_sequence\u001B[49m, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Add a batch dimension to the sample\u001B[39;00m\n\u001B[0;32m      4\u001B[0m sample_sequence \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexpand_dims(sample_sequence, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'sample_sequence' is not defined"
     ]
    }
   ],
   "source": [
    "sample_sequence = np.array(sample_sequence, dtype=np.float32)\n",
    "\n",
    "# Add a batch dimension to the sample\n",
    "sample_sequence = np.expand_dims(sample_sequence, axis=0)\n",
    "sample_sequence = torch.tensor(sample_sequence, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Model parameters (must match those used during training)\n",
    "input_size = 132   # Should be 132\n",
    "hidden_size = 128                       # As used during training\n",
    "num_layers = 2                          # As used during training\n",
    "num_classes = 5                         # Set to 5 to match the trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e95a67ebf0c157b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T16:18:35.862778Z",
     "start_time": "2024-10-03T16:18:35.754538Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "LSTM: Expected input to be 2D or 3D, got 5D instead",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m----> 2\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample_sequence\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     _, predicted_label_idx \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs\u001B[38;5;241m.\u001B[39mdata, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      4\u001B[0m     predicted_label_idx \u001B[38;5;241m=\u001B[39m predicted_label_idx\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Gymalyze\\src\\models\\lstm.py:18\u001B[0m, in \u001B[0;36mExerciseLSTM.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     15\u001B[0m h0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers, x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_size)\u001B[38;5;241m.\u001B[39mto(x\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m     16\u001B[0m c0 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers, x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_size)\u001B[38;5;241m.\u001B[39mto(x\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m---> 18\u001B[0m out, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mh0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc0\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m out \u001B[38;5;241m=\u001B[39m out[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]\n\u001B[0;32m     21\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(out)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:882\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m    880\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m):\n\u001B[1;32m--> 882\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLSTM: Expected input to be 2D or 3D, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mD instead\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    883\u001B[0m     is_batched \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m\n\u001B[0;32m    884\u001B[0m     batch_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mValueError\u001B[0m: LSTM: Expected input to be 2D or 3D, got 5D instead"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(sample_sequence)\n",
    "    _, predicted_label_idx = torch.max(outputs.data, 1)\n",
    "    predicted_label_idx = predicted_label_idx.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fafede71bbf6204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e39e347a6fd56bed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T11:47:30.997267Z",
     "start_time": "2024-10-04T11:46:50.275751Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\barrt\\AppData\\Local\\Temp\\ipykernel_27216\\1313487442.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\utils\\exercise_lstm_model_filtered.pth', map_location='cpu'))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VideoProcessor' object has no attribute 'classify_video'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 47\u001B[0m\n\u001B[0;32m     44\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mbarrt\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mPycharmProjects\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mGymalyze\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124msrc\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mutils\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mexercise_lstm_model_filtered.pth\u001B[39m\u001B[38;5;124m'\u001B[39m, map_location\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m     45\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m---> 47\u001B[0m \u001B[43mvideo_proccesor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclassify_video\u001B[49m(model, sample_sequence)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'VideoProcessor' object has no attribute 'classify_video'"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from src.models.lstm import ExerciseLSTM\n",
    "from src.utils.pose_estimator import PoseEstimator\n",
    "from src.utils.video_processor import VideoProcessor\n",
    "\n",
    "# ---------------------- Load Data ----------------------\n",
    "\n",
    "# Load the flattened sequences and labels\n",
    "X = np.load(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\landmarks_data.npy', allow_pickle=True)  # Shape: (num_samples, seq_length, input_size)\n",
    "# y = np.load('labels_data.npy')  # Shape: (num_samples,)\n",
    "\n",
    "# Load the label mapping (ensure it's the one used during training)\n",
    "with open(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\label_mapping.json', 'r') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "# Create an inverse label mapping (from indices to class names)\n",
    "inverse_label_mapping = {int(v): k for k, v in label_mapping.items()}\n",
    "\n",
    "# ---------------------- Select a Random Sample ----------------------\n",
    "\n",
    "# Select a random index\n",
    "# idx = random.randint(0, len(X) - 1)\n",
    "# sample_sequence = X[idx]  # Shape: (sequence_length, input_size)\n",
    "# true_label = y[idx]  # This is the numerical label (should be between 0 and 4)\n",
    "\n",
    "# ---------------------- Preprocess the Sample ----------------------p\n",
    "poseestimator = PoseEstimator()\n",
    "video_proccesor = VideoProcessor()\n",
    "sample_sequence = video_proccesor.process_video(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\videos\\test\\test\\squat\\squat_1.mp4')\n",
    "# Ensure the sample is a NumPy array of type float32\n",
    "sample_sequence_for_testing = deepcopy(sample_sequence)\n",
    "\n",
    "input_size = 132   # Should be 132\n",
    "hidden_size = 128                       # As used during training\n",
    "num_layers = 2                          # As used during training\n",
    "num_classes = 5  \n",
    "model = ExerciseLSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "model.load_state_dict(torch.load(r'/src/utils/lstm_v1.pth', map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "video_proccesor.classify_video(model, sample_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a276c69bbf8d8568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T11:47:41.968525Z",
     "start_time": "2024-10-04T11:47:41.954974Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barrt\\AppData\\Local\\Temp\\ipykernel_27216\\54138833.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\utils\\exercise_lstm_model_filtered.pth', map_location='cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExerciseLSTM(\n",
       "  (lstm): LSTM(132, 128, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "\n",
    "input_size = 132   # Should be 132\n",
    "hidden_size = 128                       # As used during training\n",
    "num_layers = 2                          # As used during training\n",
    "num_classes = 5  \n",
    "model = ExerciseLSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "model.load_state_dict(torch.load(r'/src/utils/lstm_v1.pth', map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d07426db17cdbbff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T11:00:15.799221Z",
     "start_time": "2024-10-04T11:00:15.790159Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "def classify_sequence(model: torch.nn.Module, sequence: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Predict the exercise class for the given sequence using the model.\n",
    "    \"\"\"\n",
    "    sequence = np.expand_dims(sequence, axis=0)\n",
    "    sequence = torch.tensor(sequence, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sequence)\n",
    "        return torch.nn.functional.softmax(outputs, dim=1).numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4268d87fab61f27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T18:25:00.172171Z",
     "start_time": "2024-10-05T18:25:00.163776Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (video_processor.py, line 203)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001B[1;36m(most recent call last)\u001B[0m:\n",
      "\u001B[0m  File \u001B[0;32m~\\PycharmProjects\\Gymalyze\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001B[0m in \u001B[0;35mrun_code\u001B[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001B[0m\n",
      "\u001B[1;36m  Cell \u001B[1;32mIn[1], line 1\u001B[1;36m\n\u001B[1;33m    from video_processor import VideoProcessor\u001B[1;36m\n",
      "\u001B[1;36m  File \u001B[1;32m~\\PycharmProjects\\Gymalyze\\src\\utils\\video_processor.py:203\u001B[1;36m\u001B[0m\n\u001B[1;33m    if\u001B[0m\n\u001B[1;37m      ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from video_processor import VideoProcessor\n",
    "video_proccesor = VideoProcessor()\n",
    "# sample_sequence = video_proccesor.process_video(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\videos\\test\\test\\squat\\squat_1.mp4')\n",
    "# sample_sequence = video_proccesor.process_video(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\videos\\test\\test\\barbell biceps curl\\video1.mp4')\n",
    "sample_sequence = video_proccesor.process_video(r\"C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\videos\\test\\test\\bench press\\bench press_2.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4f18467996e08cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T11:05:39.732118Z",
     "start_time": "2024-10-04T11:05:39.671965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93101114 0.03233624 0.02919892 0.00307928 0.00437451]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_sequence(model, sample_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2bbe9870d7ba397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T11:48:37.087350Z",
     "start_time": "2024-10-04T11:48:37.024588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93101114, 0.03233624, 0.02919892, 0.00307928, 0.00437451],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_proccesor.classify_sequence(model, sample_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
