{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-22T19:37:44.559224Z",
     "start_time": "2024-11-22T19:37:34.047127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from src.models.lstm import ExerciseLSTM\n",
    "from src.data.exercise_data import ExerciseDataset\n",
    "import numpy as np\n",
    "\n",
    "X = np.load(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\landmarks_data.npy', allow_pickle=True)\n",
    "y = np.load(r'C:\\Users\\barrt\\PycharmProjects\\Gymalyze\\src\\data\\labels_data.npy',    allow_pickle=True)\n",
    "\n",
    "print(f\"Loaded X shape: {X.shape}\")\n",
    "print(f\"Loaded y shape: {y.shape}\")\n",
    "\n",
    "if y.ndim == 2 and y.shape[1] == 1:\n",
    "    y = y.reshape(-1)\n",
    "    \n",
    "print(f\"Loaded X shape: {X.shape}\")\n",
    "print(f\"Loaded y shape: {y.shape}\")\n",
    "\n",
    "labels_to_keep = [0, 1, 4, 7, 9]\n",
    "mask = np.isin(y, labels_to_keep)\n",
    "X_filtered = X[mask]\n",
    "y_filtered = y[mask]\n",
    "\n",
    "print(f\"Filtered X shape: {X_filtered.shape}\")\n",
    "print(f\"Filtered y shape: {y_filtered.shape}\")\n",
    "print(f\"Unique labels in filtered data: {np.unique(y_filtered)}\")\n",
    "label_mapping = {original_label: new_label for new_label, original_label in enumerate(labels_to_keep)}\n",
    "y_mapped = np.array([label_mapping[label] for label in y_filtered], dtype=np.int64)\n",
    "print(f\"Mapped labels: {np.unique(y_mapped)}\")\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filtered, y_mapped, test_size=0.2, random_state=42, stratify=y_mapped)\n",
    "\n",
    "train_dataset = ExerciseDataset(X_train, y_train)\n",
    "test_dataset = ExerciseDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define hyperparameters\n",
    "hidden_sizes = [128, 256, 512]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "num_layers = [1]\n",
    "num_epochs = 20\n",
    "\n",
    "# File to save results\n",
    "results_file = \"hyperparameter_tuning_progress.csv\"\n",
    "\n",
    "# Load existing results\n",
    "if os.path.exists(results_file):\n",
    "    results_df = pd.read_csv(results_file)\n",
    "    results = results_df.to_dict(\"records\")\n",
    "else:\n",
    "    results = []\n",
    "\n",
    "# Helper to check if hyperparameters have already been processed\n",
    "def is_completed(hidden_size, lr, layers):\n",
    "    return any(\n",
    "        res[\"hidden_size\"] == hidden_size and \n",
    "        res[\"learning_rate\"] == lr and \n",
    "        res[\"num_layers\"] == layers\n",
    "        for res in results\n",
    "    )\n",
    "\n",
    "# Iterate over hyperparameters\n",
    "for hidden_size in hidden_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for layers in num_layers:\n",
    "            # Skip if this configuration is already processed\n",
    "            if is_completed(hidden_size, lr, layers):\n",
    "                print(f\"Skipping already completed: hidden_size={hidden_size}, learning_rate={lr}, num_layers={layers}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing: hidden_size={hidden_size}, learning_rate={lr}, num_layers={layers}\")\n",
    "            \n",
    "            # Define model\n",
    "            model = ExerciseLSTM(132, hidden_size, layers, len(labels_to_keep))\n",
    "            model.to(device)\n",
    "            \n",
    "            # Define criterion and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            # Track learning curves\n",
    "            epoch_losses = []\n",
    "            epoch_accuracies = []\n",
    "            epoch_precisions = []\n",
    "\n",
    "            # Train the model\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                total_loss = 0\n",
    "                correct_train = 0\n",
    "                total_train = 0\n",
    "\n",
    "                for sequences, labels in train_loader:\n",
    "                    sequences, labels = sequences.to(device), labels.to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(sequences)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Track loss\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    # Track accuracy during training\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_train += labels.size(0)\n",
    "                    correct_train += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Calculate metrics for this epoch\n",
    "                train_accuracy = 100 * correct_train / total_train\n",
    "                epoch_losses.append(total_loss / len(train_loader))\n",
    "                epoch_accuracies.append(train_accuracy)\n",
    "\n",
    "                # Evaluate precision\n",
    "                all_labels = []\n",
    "                all_predictions = []\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for sequences, labels in test_loader:\n",
    "                        sequences, labels = sequences.to(device), labels.to(device)\n",
    "                        outputs = model(sequences)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        all_labels.extend(labels.cpu().numpy())\n",
    "                        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "                precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "                epoch_precisions.append(precision)\n",
    "\n",
    "            # Evaluate final test accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for sequences, labels in test_loader:\n",
    "                    sequences, labels = sequences.to(device), labels.to(device)\n",
    "                    outputs = model(sequences)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "            test_accuracy = 100 * correct / total\n",
    "            result = {\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"learning_rate\": lr,\n",
    "                \"num_layers\": layers,\n",
    "                \"test_accuracy\": test_accuracy,\n",
    "                \"epoch_losses\": epoch_losses,\n",
    "                \"epoch_accuracies\": epoch_accuracies,\n",
    "                \"epoch_precisions\": epoch_precisions,\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            print(f\"Final Test Accuracy: {test_accuracy:.2f}%\\n\")\n",
    "\n",
    "            # Save results after each iteration\n",
    "            results_df = pd.DataFrame(results)\n",
    "            results_df.to_csv(results_file, index=False)\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X shape: (761, 300, 132)\n",
      "Loaded y shape: (761, 1)\n",
      "Loaded X shape: (761, 300, 132)\n",
      "Loaded y shape: (761,)\n",
      "Filtered X shape: (227, 300, 132)\n",
      "Filtered y shape: (227,)\n",
      "Unique labels in filtered data: [0 1 4 7 9]\n",
      "Mapped labels: [0 1 2 3 4]\n",
      "Using device: cuda\n",
      "Skipping already completed: hidden_size=128, learning_rate=0.01, num_layers=1\n",
      "Skipping already completed: hidden_size=128, learning_rate=0.001, num_layers=1\n",
      "Skipping already completed: hidden_size=128, learning_rate=0.0001, num_layers=1\n",
      "Skipping already completed: hidden_size=256, learning_rate=0.01, num_layers=1\n",
      "Skipping already completed: hidden_size=256, learning_rate=0.001, num_layers=1\n",
      "Skipping already completed: hidden_size=256, learning_rate=0.0001, num_layers=1\n",
      "Skipping already completed: hidden_size=512, learning_rate=0.01, num_layers=1\n",
      "Skipping already completed: hidden_size=512, learning_rate=0.001, num_layers=1\n",
      "Skipping already completed: hidden_size=512, learning_rate=0.0001, num_layers=1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3731b64f20efac41"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
